1. Positional encoding을 learnable하게도, fixed vector로도 작성 가능하다. 
다만 positional encoding은 input sequence의 step 개념을 explicit하게 대신하는 vector이기에, 입력되는 sequence는 정상적인 order로 입력되어야 한다.

2. Transformer는 input sequence 전체를 보고 output을 도출하는 방식이다. 따라서 지나치게 긴 input sequence가 입력되는 경우 계산 비용에서 비효율적일 수 있다.
또한 Transformer의 attention mechanism은 전체 입력 sequence로부터 softmax로부터 도출된 weighting을 주는 방식이기에, 길이가 너무 길면 적절한 weighting을 학습하기 어려울 수 있다.
이는 전체적인 성능의 불안정성으로 직결될 수 있다. (reference: Theoretical Limitations of Self-Attention in Neural Sequence Models) 
